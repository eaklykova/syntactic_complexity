{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0511a0e",
   "metadata": {},
   "source": [
    "# Spell-checking with Yandex Speller & Annotation with UDpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba187a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyaspeller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f63d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from conllu import parse\n",
    "from pyaspeller import YandexSpeller\n",
    "speller = YandexSpeller()\n",
    "\n",
    "ENG_MODEL = 'english-ewt-ud-2.12-230717'\n",
    "RUS_MODEL = 'russian-syntagrus-ud-2.12-230717'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9584c16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('_spell_checked'):\n",
    "    os.mkdir('_spell_checked')\n",
    "\n",
    "if not os.path.exists('annotate'):\n",
    "    os.mkdir('annotate')\n",
    "\n",
    "if not os.path.exists('_annotated'):\n",
    "    os.mkdir('_annotated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d52abda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_corpus_from_json(path):\n",
    "    df = pd.read_json(path, orient='records')\n",
    "    df.dropna(subset='text', inplace=True)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4428ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck_corpus(df, out_file):\n",
    "    spelled = []\n",
    "    for text in tqdm(df['text'].tolist()):\n",
    "        try:\n",
    "            spelled.append(speller.spelled(text))\n",
    "        except EncodingError:\n",
    "            spelled.append(text)\n",
    "    df['spell_checked'] = spelled\n",
    "    df.to_json(out_file, orient='records', force_ascii=False, indent=4)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b950f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_raw_files(texts, corpus_name, model):\n",
    "\n",
    "    ud_commands = []\n",
    "    out_files = []\n",
    "\n",
    "    if not os.path.exists(f'annotate/{corpus_name}'):\n",
    "        os.mkdir(f'annotate/{corpus_name}')\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        pad = len(str(len(texts)))\n",
    "        file_name = f'{corpus_name}/{corpus_name}{str(i).zfill(pad)}'\n",
    "        with open(f'annotate/{file_name}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        command = f'curl -F data=@{file_name}.txt -F model=\"{model}\" -F tokenizer=\"{model}\" -F tagger=\"{model}\" -F parser=\"{model}\" -F output=conllu http://lindat.mff.cuni.cz/services/udpipe/api/process > {file_name}.json'\n",
    "        ud_commands.append(command)\n",
    "        out_files.append(f'{file_name}.json')\n",
    "\n",
    "    bat_file = f'annotate/{corpus_name}.bat'\n",
    "    with open(bat_file, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(ud_commands))\n",
    "\n",
    "    return [os.path.join('annotate/', file) for file in out_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd65ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_annotated_files(corpus_name, out_files):\n",
    "    conllus = [os.path.join(f'annotate/{corpus_name}/', file)\n",
    "               for file in os.listdir(f'annotate/{corpus_name}')\n",
    "               if file.endswith('.json')]\n",
    "    try:\n",
    "        assert len(conllus) == len(out_files)\n",
    "        print('All OK!')\n",
    "    except AssertionError:\n",
    "        print(set(out_files).difference(set(conllus)))\n",
    "\n",
    "    return conllus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efc05460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(df, conllus, out_file):\n",
    "    annos = []\n",
    "    for file in conllus:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            annos.append(json.load(f)['result'])\n",
    "    df['annotated'] = annos\n",
    "    df.to_json(out_file, orient='records', force_ascii=False, indent=4)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "216dc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(*cols):\n",
    "    for col in cols:\n",
    "        if pd.notnull(col):\n",
    "            return col\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244bc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_words(conl):\n",
    "    text_len = 0\n",
    "    for sent in conl:\n",
    "        text_len += len([word for word in sent if word['upos'] not in {'PUNCT', 'SYM', '_'}])\n",
    "    return text_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d57969a",
   "metadata": {},
   "source": [
    "### LOCNESS\n",
    "* All texts are used\n",
    "* No spell-checking\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41344129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locness = load_clean_corpus_from_json('_clean/locness.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5a14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_files = create_raw_files(locness['text'].tolist(), corpus_name='locness', model=ENG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "885415a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "# conllus = check_annotated_files('locness', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a26e4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locness = get_annotations(locness, conllus, '_annotated/locness.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c95cf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>code</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>dialect</th>\n",
       "      <th>topic</th>\n",
       "      <th>task_type</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>locness\\alevels1.txt</td>\n",
       "      <td>Transport 01</td>\n",
       "      <td>The basic dilema facing the UK's rail and road...</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>british</td>\n",
       "      <td>Transport</td>\n",
       "      <td>argumentative</td>\n",
       "      <td>a-level</td>\n",
       "      <td>None</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>locness\\alevels1.txt</td>\n",
       "      <td>Transport 02</td>\n",
       "      <td>Traffic jams are becoming larger and more freq...</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>british</td>\n",
       "      <td>Transport</td>\n",
       "      <td>argumentative</td>\n",
       "      <td>a-level</td>\n",
       "      <td>None</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>locness\\alevels1.txt</td>\n",
       "      <td>Transport 03</td>\n",
       "      <td>As transport has advanced over the past 200 ye...</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>british</td>\n",
       "      <td>Transport</td>\n",
       "      <td>argumentative</td>\n",
       "      <td>a-level</td>\n",
       "      <td>None</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               filename          code  \\\n",
       "0  locness\\alevels1.txt  Transport 01   \n",
       "1  locness\\alevels1.txt  Transport 02   \n",
       "2  locness\\alevels1.txt  Transport 03   \n",
       "\n",
       "                                                text language speaker_type  \\\n",
       "0  The basic dilema facing the UK's rail and road...  english           L1   \n",
       "1  Traffic jams are becoming larger and more freq...  english           L1   \n",
       "2  As transport has advanced over the past 200 ye...  english           L1   \n",
       "\n",
       "   dialect      topic      task_type    level location  \\\n",
       "0  british  Transport  argumentative  a-level     None   \n",
       "1  british  Transport  argumentative  a-level     None   \n",
       "2  british  Transport  argumentative  a-level     None   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locness = pd.read_json('_annotated/locness.json')\n",
    "locness.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf1c50",
   "metadata": {},
   "source": [
    "## RLC\n",
    "* All texts are used\n",
    "* Spell-checking is performed\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34a6ef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2002, 13)\n"
     ]
    }
   ],
   "source": [
    "# rlc = load_clean_corpus_from_json('_clean/rlc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21b01d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rlc = spellcheck_corpus(rlc, '_spell_checked/rlc.json')\n",
    "rlc = pd.read_json('_spell_checked/rlc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fff790a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = create_raw_files(rlc['spell_checked'].tolist(), corpus_name='rlc', model=RUS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6ef04ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK!\n"
     ]
    }
   ],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "conllus = check_annotated_files('rlc', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6413c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rlc = get_annotations(rlc, conllus, '_annotated/rlc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb483477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>corrected</th>\n",
       "      <th>status</th>\n",
       "      <th>subcorpus</th>\n",
       "      <th>native</th>\n",
       "      <th>language_background</th>\n",
       "      <th>level</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>prompt</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Загрязнение тяжелыми металлами Дальнегорского ...</td>\n",
       "      <td>Загрязнение тяжелыми металлами Дальнегорского ...</td>\n",
       "      <td>None</td>\n",
       "      <td>RULEC</td>\n",
       "      <td>eng</td>\n",
       "      <td>HL</td>\n",
       "      <td>AM</td>\n",
       "      <td>431</td>\n",
       "      <td>22</td>\n",
       "      <td>russian</td>\n",
       "      <td>L2</td>\n",
       "      <td>None</td>\n",
       "      <td>Загрязнение тяжелыми металлами Дальнегорского ...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Директору магазина « Адидас» М. И. Васильченко...</td>\n",
       "      <td>Директору магазина « Адидас» М. И. Васильченко...</td>\n",
       "      <td>None</td>\n",
       "      <td>RULEC</td>\n",
       "      <td>eng</td>\n",
       "      <td>HL</td>\n",
       "      <td>AM</td>\n",
       "      <td>245</td>\n",
       "      <td>17</td>\n",
       "      <td>russian</td>\n",
       "      <td>L2</td>\n",
       "      <td>None</td>\n",
       "      <td>Директору магазина « Адидас» М. И. Васильченко...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Вывод. Спасибо, ребята, за хорошие ответы. Я м...</td>\n",
       "      <td>Вывод. Спасибо, ребята, за хорошие ответы. Я м...</td>\n",
       "      <td>None</td>\n",
       "      <td>RULEC</td>\n",
       "      <td>eng</td>\n",
       "      <td>FL</td>\n",
       "      <td>AM</td>\n",
       "      <td>472</td>\n",
       "      <td>22</td>\n",
       "      <td>russian</td>\n",
       "      <td>L2</td>\n",
       "      <td>None</td>\n",
       "      <td>Вывод. Спасибо, ребята, за хорошие ответы. Я м...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                               text  \\\n",
       "0            1  Загрязнение тяжелыми металлами Дальнегорского ...   \n",
       "1            3  Директору магазина « Адидас» М. И. Васильченко...   \n",
       "2            5  Вывод. Спасибо, ребята, за хорошие ответы. Я м...   \n",
       "\n",
       "                                           corrected status subcorpus native  \\\n",
       "0  Загрязнение тяжелыми металлами Дальнегорского ...   None     RULEC    eng   \n",
       "1  Директору магазина « Адидас» М. И. Васильченко...   None     RULEC    eng   \n",
       "2  Вывод. Спасибо, ребята, за хорошие ответы. Я м...   None     RULEC    eng   \n",
       "\n",
       "  language_background level  words  sentences language speaker_type prompt  \\\n",
       "0                  HL    AM    431         22  russian           L2   None   \n",
       "1                  HL    AM    245         17  russian           L2   None   \n",
       "2                  FL    AM    472         22  russian           L2   None   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  Загрязнение тяжелыми металлами Дальнегорского ...   \n",
       "1  Директору магазина « Адидас» М. И. Васильченко...   \n",
       "2  Вывод. Спасибо, ребята, за хорошие ответы. Я м...   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlc = pd.read_json('_annotated/rlc.json')\n",
    "rlc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29142b",
   "metadata": {},
   "source": [
    "## RULEC\n",
    "* Only texts of type \"paragraph\" are used\n",
    "* Spell-checking is perfomed\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b99327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rulec = load_clean_corpus_from_json('_clean/rulec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d33998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_task(text, task):\n",
    "    tasks = []\n",
    "    if task:\n",
    "        tasks.append(task)\n",
    "    tasks.extend(re.findall(r'<[^a-z]+?>', text))\n",
    "    for task in tasks:\n",
    "        text = text.replace(task.strip(), '')\n",
    "    text = re.sub(r'<.+?>', '', text)\n",
    "    return text.strip(), '\\n'.join(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a085d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(text):\n",
    "    if re.match(r'.{1,85}[А-ЯA-Zа-яa-zЁё»)]\\n', text):\n",
    "        parts = text.split('\\n')\n",
    "        return parts[0], '\\n'.join(parts[1:])\n",
    "    return np.nan, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b121419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rulec = rulec[rulec['text type'] == 'paragraph']  # filter texts\n",
    "# rulec[['text', 'task']] = rulec.apply(lambda x: extract_task(x['text'], x['task']),\n",
    "#                                       axis=1, result_type='expand')\n",
    "# rulec[['title', 'text']] = rulec.apply(lambda x: extract_title(x['text']),\n",
    "#                                        axis=1, result_type='expand')\n",
    "# print(rulec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "731af141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rulec = spellcheck_corpus(rulec, '_spell_checked/rulec.json')\n",
    "# rulec = pd.read_json('_spell_checked/rulec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c4c1513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_files = create_raw_files(rulec['spell_checked'].tolist(), corpus_name='rulec', model=RUS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7466b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "# conllus = check_annotated_files('rulec', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f2b472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rulec = get_annotations(rulec, conllus, '_annotated/rulec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6e1d88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>student</th>\n",
       "      <th>gender</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>level</th>\n",
       "      <th>year</th>\n",
       "      <th>course</th>\n",
       "      <th>week</th>\n",
       "      <th>text type</th>\n",
       "      <th>function</th>\n",
       "      <th>time</th>\n",
       "      <th>mode</th>\n",
       "      <th>language</th>\n",
       "      <th>task</th>\n",
       "      <th>title</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anna_HL_2009-2010_Week_19_2_paragraph+_descrip...</td>\n",
       "      <td>Торговля людьми- популярная, широко обсуждаема...</td>\n",
       "      <td>Anna</td>\n",
       "      <td>f</td>\n",
       "      <td>HL</td>\n",
       "      <td>AL</td>\n",
       "      <td>2009-2010</td>\n",
       "      <td>Russian In The Major</td>\n",
       "      <td>192</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>description</td>\n",
       "      <td>non-timed</td>\n",
       "      <td>individual</td>\n",
       "      <td>russian</td>\n",
       "      <td>&lt;Представьте термин из области вашей специаль...</td>\n",
       "      <td>Торговля людьми</td>\n",
       "      <td>Торговля людьми- популярная, широко обсуждаема...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna_HL_AL_2009-2010_Week_12_1_paragraph_summa...</td>\n",
       "      <td>Автор статьи излагает положительные и отрицате...</td>\n",
       "      <td>Anna</td>\n",
       "      <td>f</td>\n",
       "      <td>HL</td>\n",
       "      <td>AL</td>\n",
       "      <td>2009-2010</td>\n",
       "      <td>American Studies</td>\n",
       "      <td>121</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>summary</td>\n",
       "      <td>non-timed</td>\n",
       "      <td>individual</td>\n",
       "      <td>russian</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Автор статьи излагает положительные и отрицате...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anna_HL_AL_2009-2010_Week_12_2_paragraph_suppo...</td>\n",
       "      <td>Я считаю, что Сталин любил этот стил архитекту...</td>\n",
       "      <td>Anna</td>\n",
       "      <td>f</td>\n",
       "      <td>HL</td>\n",
       "      <td>AL</td>\n",
       "      <td>2009-2010</td>\n",
       "      <td>European Studies</td>\n",
       "      <td>122</td>\n",
       "      <td>paragraph</td>\n",
       "      <td>supported opinion</td>\n",
       "      <td>timed, 10 min</td>\n",
       "      <td>individual</td>\n",
       "      <td>russian</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>Я считаю, что Сталин любил этот стиль архитект...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  Anna_HL_2009-2010_Week_19_2_paragraph+_descrip...   \n",
       "1  Anna_HL_AL_2009-2010_Week_12_1_paragraph_summa...   \n",
       "2  Anna_HL_AL_2009-2010_Week_12_2_paragraph_suppo...   \n",
       "\n",
       "                                                text student gender  \\\n",
       "0  Торговля людьми- популярная, широко обсуждаема...    Anna      f   \n",
       "1  Автор статьи излагает положительные и отрицате...    Anna      f   \n",
       "2  Я считаю, что Сталин любил этот стил архитекту...    Anna      f   \n",
       "\n",
       "  speaker_type level       year                course  week  text type  \\\n",
       "0           HL    AL  2009-2010  Russian In The Major   192  paragraph   \n",
       "1           HL    AL  2009-2010      American Studies   121  paragraph   \n",
       "2           HL    AL  2009-2010      European Studies   122  paragraph   \n",
       "\n",
       "            function           time        mode language  \\\n",
       "0        description      non-timed  individual  russian   \n",
       "1            summary      non-timed  individual  russian   \n",
       "2  supported opinion  timed, 10 min  individual  russian   \n",
       "\n",
       "                                                task            title  \\\n",
       "0  <Представьте термин из области вашей специаль...  Торговля людьми   \n",
       "1                                                                None   \n",
       "2                                                                None   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  Торговля людьми- популярная, широко обсуждаема...   \n",
       "1  Автор статьи излагает положительные и отрицате...   \n",
       "2  Я считаю, что Сталин любил этот стиль архитект...   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rulec = pd.read_json('_annotated/rulec.json')\n",
    "rulec.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e85d3e",
   "metadata": {},
   "source": [
    "## REALEC\n",
    "* Only texts with a specified task (graph/essay) are used\n",
    "* Only texts with either a grade or a CEFR level are used\n",
    "* Spell-checking is performed\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baaf56c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realec = load_clean_corpus_from_json('_clean/realec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d222e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter texts\n",
    "# realec = realec[((realec['mark'].notnull()) | (realec['CEFR_level'].notnull())) &\n",
    "#                 (realec['text_type'].notnull())]\n",
    "# print(realec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f06a6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realec = spellcheck_corpus(realec, '_spell_checked/realec.json')\n",
    "# realec = pd.read_json('_spell_checked/realec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc824e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_files = create_raw_files(realec['spell_checked'].tolist(), corpus_name='realec', model=ENG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f428a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "# conllus = check_annotated_files('realec', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a89067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# realec = get_annotations(realec, conllus, '_annotated/realec.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8be2871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>annotation</th>\n",
       "      <th>text</th>\n",
       "      <th>sex</th>\n",
       "      <th>mark</th>\n",
       "      <th>study_year</th>\n",
       "      <th>date</th>\n",
       "      <th>ielts</th>\n",
       "      <th>work_type</th>\n",
       "      <th>text_type</th>\n",
       "      <th>ann_checked</th>\n",
       "      <th>CEFR_level</th>\n",
       "      <th>year</th>\n",
       "      <th>essay_title</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>esl_00011.ann</td>\n",
       "      <td>T3\\tAbsence_comp_sent 279 283\\tdeal\\n#3\\tAnnot...</td>\n",
       "      <td>This episode is about a very interesting case ...</td>\n",
       "      <td>f</td>\n",
       "      <td>80</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2016-10-10</td>\n",
       "      <td>1</td>\n",
       "      <td>exam</td>\n",
       "      <td>opinion essay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>L2</td>\n",
       "      <td>Tus episode is about a very interesting case i...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAl_10_1.ann</td>\n",
       "      <td>T1\\tAgreement_errors 14 23\\tvisualize\\nT4\\tRed...</td>\n",
       "      <td>Given diagram visualize the proportion of popu...</td>\n",
       "      <td>m</td>\n",
       "      <td>55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2014-03-28</td>\n",
       "      <td>1</td>\n",
       "      <td>exam</td>\n",
       "      <td>graph description</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>L2</td>\n",
       "      <td>Given diagram visualize the proportion of popu...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAl_10_2.ann</td>\n",
       "      <td>T1\\tSpelling 389 391\\tan\\nT2\\tSpelling 407 410...</td>\n",
       "      <td>I strongly disagree with the given assumption ...</td>\n",
       "      <td>m</td>\n",
       "      <td>65</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2014-03-28</td>\n",
       "      <td>1</td>\n",
       "      <td>exam</td>\n",
       "      <td>opinion essay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>english</td>\n",
       "      <td>L2</td>\n",
       "      <td>I strongly disagree with the given assumption ...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                         annotation  \\\n",
       "0  esl_00011.ann  T3\\tAbsence_comp_sent 279 283\\tdeal\\n#3\\tAnnot...   \n",
       "1   AAl_10_1.ann  T1\\tAgreement_errors 14 23\\tvisualize\\nT4\\tRed...   \n",
       "2   AAl_10_2.ann  T1\\tSpelling 389 391\\tan\\nT2\\tSpelling 407 410...   \n",
       "\n",
       "                                                text sex  mark  study_year  \\\n",
       "0  This episode is about a very interesting case ...   f    80         4.0   \n",
       "1  Given diagram visualize the proportion of popu...   m    55         2.0   \n",
       "2  I strongly disagree with the given assumption ...   m    65         2.0   \n",
       "\n",
       "        date  ielts work_type          text_type  ann_checked  CEFR_level  \\\n",
       "0 2016-10-10      1      exam      opinion essay          NaN         NaN   \n",
       "1 2014-03-28      1      exam  graph description          NaN         NaN   \n",
       "2 2014-03-28      1      exam      opinion essay          NaN         NaN   \n",
       "\n",
       "   year  essay_title language speaker_type  \\\n",
       "0   NaN          NaN  english           L2   \n",
       "1   NaN          NaN  english           L2   \n",
       "2   NaN          NaN  english           L2   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  Tus episode is about a very interesting case i...   \n",
       "1  Given diagram visualize the proportion of popu...   \n",
       "2  I strongly disagree with the given assumption ...   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realec = pd.read_json('_annotated/realec.json')\n",
    "realec.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbaeaf",
   "metadata": {},
   "source": [
    "## ACTR\n",
    "* Group work is excluded\n",
    "* Spell-checking is performed\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c79c9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actr = load_clean_corpus_from_json('_clean/actr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ecf2a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_actr_title(text):\n",
    "    parts = text.split('\\n')\n",
    "    first = parts[0]\n",
    "    if len(first) < 100 and not re.search('^-?.+?[,.]$', first):\n",
    "        return first, '\\n'.join(parts[1:])\n",
    "    elif len(first) < 46 and not first.endswith(','):\n",
    "        return first, '\\n'.join(parts[1:])\n",
    "    return np.nan, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6fe6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actr[['title', 'text']] = actr.apply(lambda x: extract_actr_title(x['text']),\n",
    "#                                      axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c486294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actr = spellcheck_corpus(actr, '_spell_checked/actr.json')\n",
    "# actr = pd.read_json('_spell_checked/actr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef1a8521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_files = create_raw_files(actr['spell_checked'].tolist(), corpus_name='actr', model=RUS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bb4bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "# conllus = check_annotated_files('actr', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "999129d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actr = get_annotations(actr, conllus, '_annotated/actr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a83b4704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>raw_name</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>level</th>\n",
       "      <th>age</th>\n",
       "      <th>prompt</th>\n",
       "      <th>institution</th>\n",
       "      <th>...</th>\n",
       "      <th>native_lang</th>\n",
       "      <th>name</th>\n",
       "      <th>school_grade</th>\n",
       "      <th>dob</th>\n",
       "      <th>date</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>title</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...</td>\n",
       "      <td>hs1_50</td>\n",
       "      <td>Прокатившись по многим городам и странам за по...</td>\n",
       "      <td>HS1-50</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>\"Место Любимое Моё\"</td>\n",
       "      <td>Прокатившись по многим городам и странам за по...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...</td>\n",
       "      <td>hs1_30</td>\n",
       "      <td>В моей жизни я встречала много разных людей. У...</td>\n",
       "      <td>HS1-30</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>None</td>\n",
       "      <td>В моей жизни я встречала много разных людей. У...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...</td>\n",
       "      <td>hs1_24</td>\n",
       "      <td>В прошлем году, я провела четыре месяцев в Лон...</td>\n",
       "      <td>HS1-24</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>None</td>\n",
       "      <td>В прошлом году, я провела четыре месяцев в Лон...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path raw_name  \\\n",
       "0  actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...   hs1_50   \n",
       "1  actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...   hs1_30   \n",
       "2  actr\\Zhenyas data\\Essay Contest Heritage 1\\Num...   hs1_24   \n",
       "\n",
       "                                                text speaker_id country  \\\n",
       "0  Прокатившись по многим городам и странам за по...     HS1-50    None   \n",
       "1  В моей жизни я встречала много разных людей. У...     HS1-30    None   \n",
       "2  В прошлем году, я провела четыре месяцев в Лон...     HS1-24    None   \n",
       "\n",
       "  gender       level  age prompt institution  ... native_lang  name  \\\n",
       "0   None  Heritage 1  NaN   None        None  ...        None  None   \n",
       "1   None  Heritage 1  NaN   None        None  ...        None  None   \n",
       "2   None  Heritage 1  NaN   None        None  ...        None  None   \n",
       "\n",
       "  school_grade  dob  date language speaker_type                 title  \\\n",
       "0          NaN  NaN   NaT  russian           L1  \"Место Любимое Моё\"   \n",
       "1          NaN  NaN   NaT  russian           L1                  None   \n",
       "2          NaN  NaN   NaT  russian           L1                  None   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  Прокатившись по многим городам и странам за по...   \n",
       "1  В моей жизни я встречала много разных людей. У...   \n",
       "2  В прошлом году, я провела четыре месяцев в Лон...   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actr = pd.read_json('_annotated/actr.json')\n",
    "actr.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee268b8b",
   "metadata": {},
   "source": [
    "## CoRST / КРУТ\n",
    "* Only texts of certain types are used\n",
    "* No spell-checking\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7bef762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corst = load_clean_corpus_from_json('_clean/corst.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37237b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter texts\n",
    "# corst = corst[corst['genre'].isin({'аннотация', 'аннотация проекта', 'эссе',\n",
    "#                                    'коммерческое предложение', 'семестровая работа'})]\n",
    "# print(corst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b8a2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_files = create_raw_files(corst['text'].tolist(), corpus_name='corst', model=RUS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3deae59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "# conllus = check_annotated_files('corst', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f17396f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corst = get_annotations(corst, conllus, '_annotated/corst.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb3f7fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>created</th>\n",
       "      <th>author</th>\n",
       "      <th>date1</th>\n",
       "      <th>date2</th>\n",
       "      <th>genre</th>\n",
       "      <th>gender</th>\n",
       "      <th>major</th>\n",
       "      <th>course</th>\n",
       "      <th>term</th>\n",
       "      <th>module</th>\n",
       "      <th>domain</th>\n",
       "      <th>university</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>задание№ 1 В самом раннем детстве меня привлек...</td>\n",
       "      <td>1435876038000</td>\n",
       "      <td>Судакова Мария</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>эссе</td>\n",
       "      <td>f</td>\n",
       "      <td>Дизайн</td>\n",
       "      <td>2 курс бак</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>разнородная тематика</td>\n",
       "      <td>None</td>\n",
       "      <td>518</td>\n",
       "      <td>42</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Многие называют Мальту игрушечной страной. И п...</td>\n",
       "      <td>1435876038000</td>\n",
       "      <td>Бардина Анастасия</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>эссе</td>\n",
       "      <td>f</td>\n",
       "      <td>Дизайн</td>\n",
       "      <td>2 курс бак</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>разнородная тематика</td>\n",
       "      <td>None</td>\n",
       "      <td>291</td>\n",
       "      <td>23</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>Так , что же такое дизайн вообще и дизайн сред...</td>\n",
       "      <td>1435876038000</td>\n",
       "      <td>Калашян Юлия</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>эссе</td>\n",
       "      <td>f</td>\n",
       "      <td>Дизайн</td>\n",
       "      <td>2 курс бак</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>разнородная тематика</td>\n",
       "      <td>None</td>\n",
       "      <td>535</td>\n",
       "      <td>24</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                               text        created  \\\n",
       "0       2  задание№ 1 В самом раннем детстве меня привлек...  1435876038000   \n",
       "1       5  Многие называют Мальту игрушечной страной. И п...  1435876038000   \n",
       "2       6  Так , что же такое дизайн вообще и дизайн сред...  1435876038000   \n",
       "\n",
       "              author   date1   date2 genre gender   major      course  term  \\\n",
       "0     Судакова Мария  2013.0  2014.0  эссе      f  Дизайн  2 курс бак   2.0   \n",
       "1  Бардина Анастасия  2013.0  2014.0  эссе      f  Дизайн  2 курс бак   2.0   \n",
       "2       Калашян Юлия  2013.0  2014.0  эссе      f  Дизайн  2 курс бак   2.0   \n",
       "\n",
       "   module                domain university  words  sentences language  \\\n",
       "0     4.0  разнородная тематика       None    518         42  russian   \n",
       "1     4.0  разнородная тематика       None    291         23  russian   \n",
       "2     4.0  разнородная тематика       None    535         24  russian   \n",
       "\n",
       "  speaker_type                                          annotated  \n",
       "0           L1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1           L1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2           L1  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corst = pd.read_json('_annotated/corst.json')\n",
    "corst.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27d2e4",
   "metadata": {},
   "source": [
    "## Combining dataframes to estimate length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb5d90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_whole_corpus(texts):\n",
    "    return [parse(text) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5287451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len_words(conl):\n",
    "    text_len = 0\n",
    "    for sent in conl:\n",
    "        text_len += len(sent)\n",
    "    return text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32aaecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_annotated/actr.json',\n",
       " '_annotated/corst.json',\n",
       " '_annotated/locness.json',\n",
       " '_annotated/realec.json',\n",
       " '_annotated/rlc.json',\n",
       " '_annotated/rulec.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = [os.path.join('_annotated/', file) for file in os.listdir('_annotated')\n",
    "           if 'reddit' not in file and 'pikabu' not in file]\n",
    "corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1807420c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a68cc3ed2a34b6e8540fc6adf6b3137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corp_dfs = []\n",
    "for file in tqdm(corpora):\n",
    "    corpus = pd.read_json(file, orient='records')\n",
    "    corpus['corpus'], _ = os.path.splitext(os.path.basename(file))\n",
    "    corp_dfs.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed0bf456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9403, 63)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat(corp_dfs)\n",
    "data.dropna(how='all', axis=0, inplace=True)\n",
    "data.dropna(how='all', axis=1, inplace=True)\n",
    "data.fillna(np.nan, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd3eb6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9403, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>subcorpus</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus subcorpus language speaker_type  \\\n",
       "0   actr       NaN  russian           L1   \n",
       "1   actr       NaN  russian           L1   \n",
       "2   actr       NaN  russian           L1   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['corpus', 'subcorpus', 'language', 'speaker_type', 'annotated']]\n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c171f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9e63cb6a604316b8d4a5a80bc56ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['conllu'] = parse_whole_corpus(data['annotated'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5a81ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    9403.000000\n",
       "mean      351.011486\n",
       "std       377.599445\n",
       "min         0.000000\n",
       "25%       165.000000\n",
       "50%       254.000000\n",
       "75%       380.000000\n",
       "max      7931.000000\n",
       "Name: num_words, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['num_words'] = data['conllu'].map(get_len_words)\n",
    "data['num_words'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b5080e",
   "metadata": {},
   "source": [
    "### Pikabu\n",
    "* 20k texts with similar length distribution\n",
    "* No spell-checking\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "efcfc05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffb3209840e4bcf96590159b34c24ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3005828, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>author_id</th>\n",
       "      <th>pluses</th>\n",
       "      <th>minuses</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6991642</td>\n",
       "      <td>https://pikabu.ru/story/chto_mozhno_kupit_v_ki...</td>\n",
       "      <td>Что можно купить в Китае за цену нового iPhone...</td>\n",
       "      <td>Осенью в России стартовали продажи очередной м...</td>\n",
       "      <td>1571221527</td>\n",
       "      <td>2900955</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7178566</td>\n",
       "      <td>https://pikabu.ru/story/posledniy_ostavshiysya...</td>\n",
       "      <td>Последний оставшийся в живых освободитель Осве...</td>\n",
       "      <td>В канун 75-летия освобождения концлагеря и V В...</td>\n",
       "      <td>1579586602</td>\n",
       "      <td>1723707</td>\n",
       "      <td>1498</td>\n",
       "      <td>159</td>\n",
       "      <td>1690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7021067</td>\n",
       "      <td>https://pikabu.ru/story/zima_v_tyumen_prishla_...</td>\n",
       "      <td>Зима в Тюмень пришла.</td>\n",
       "      <td>И в честь этого я сочинил свой первый пирожок....</td>\n",
       "      <td>1572537738</td>\n",
       "      <td>2473821</td>\n",
       "      <td>517</td>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                                url  \\\n",
       "0  6991642  https://pikabu.ru/story/chto_mozhno_kupit_v_ki...   \n",
       "1  7178566  https://pikabu.ru/story/posledniy_ostavshiysya...   \n",
       "2  7021067  https://pikabu.ru/story/zima_v_tyumen_prishla_...   \n",
       "\n",
       "                                               title  \\\n",
       "0  Что можно купить в Китае за цену нового iPhone...   \n",
       "1  Последний оставшийся в живых освободитель Осве...   \n",
       "2                              Зима в Тюмень пришла.   \n",
       "\n",
       "                                                text        date  author_id  \\\n",
       "0  Осенью в России стартовали продажи очередной м...  1571221527    2900955   \n",
       "1  В канун 75-летия освобождения концлагеря и V В...  1579586602    1723707   \n",
       "2  И в честь этого я сочинил свой первый пирожок....  1572537738    2473821   \n",
       "\n",
       "   pluses  minuses  num_words  \n",
       "0       9       13       1029  \n",
       "1    1498      159       1690  \n",
       "2     517       64         18  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [chunk for chunk in tqdm(\n",
    "    pd.read_csv('_clean/pikabu_large.tsv', sep='\\t', chunksize=500000))]\n",
    "pikabu = pd.concat(chunks)\n",
    "print(pikabu.shape)\n",
    "pikabu.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "094a1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pikabu['corpus'] = 'pikabu'\n",
    "pikabu['language'] = 'russian'\n",
    "pikabu['speaker_type'] = 'L1'\n",
    "pikabu.drop(columns=['pluses', 'minuses', 'url'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dac0ab8",
   "metadata": {},
   "source": [
    "### Reddit\n",
    "* 20k texts with similar length distribution\n",
    "* No spell-checking\n",
    "* Annotation DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21ca145c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666cb470df5e462dab74d4a0d1015b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1763029, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>content_len</th>\n",
       "      <th>summary_len</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t5_2qore</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "      <td>You are talking about the Charsi imbue, right?...</td>\n",
       "      <td>D2 help?</td>\n",
       "      <td>c6acxvc</td>\n",
       "      <td>Class only items dropped from high-lvl monsters.</td>\n",
       "      <td>Diablo</td>\n",
       "      <td>NuffZetPand0ra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t5_2qore</td>\n",
       "      <td>96</td>\n",
       "      <td>23</td>\n",
       "      <td>Change out force armor for pinpoint barrier (+...</td>\n",
       "      <td>Initial Impressions of CM Wizards in 1.0.5</td>\n",
       "      <td>c6dvxzf</td>\n",
       "      <td>CC, LoH, and APoC before DPS (in my opinion). ...</td>\n",
       "      <td>Diablo</td>\n",
       "      <td>zlevine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t5_2qore</td>\n",
       "      <td>380</td>\n",
       "      <td>46</td>\n",
       "      <td>Guess I'll throw my .02 in here. Spec(Spectral...</td>\n",
       "      <td>Initial Impressions of CM Wizards in 1.0.5</td>\n",
       "      <td>c6dm81w</td>\n",
       "      <td>Spectral Blades - Deep Cuts still procs CM fro...</td>\n",
       "      <td>Diablo</td>\n",
       "      <td>Nekenieh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit_id  content_len  summary_len  \\\n",
       "0     t5_2qore          404            7   \n",
       "1     t5_2qore           96           23   \n",
       "2     t5_2qore          380           46   \n",
       "\n",
       "                                                text  \\\n",
       "0  You are talking about the Charsi imbue, right?...   \n",
       "1  Change out force armor for pinpoint barrier (+...   \n",
       "2  Guess I'll throw my .02 in here. Spec(Spectral...   \n",
       "\n",
       "                                        title   doc_id  \\\n",
       "0                                    D2 help?  c6acxvc   \n",
       "1  Initial Impressions of CM Wizards in 1.0.5  c6dvxzf   \n",
       "2  Initial Impressions of CM Wizards in 1.0.5  c6dm81w   \n",
       "\n",
       "                                              prompt subreddit       author_id  \n",
       "0   Class only items dropped from high-lvl monsters.    Diablo  NuffZetPand0ra  \n",
       "1  CC, LoH, and APoC before DPS (in my opinion). ...    Diablo         zlevine  \n",
       "2  Spectral Blades - Deep Cuts still procs CM fro...    Diablo        Nekenieh  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = [chunk for chunk in tqdm(\n",
    "    pd.read_csv('_clean/reddit_large.tsv', sep='\\t', chunksize=500000))]\n",
    "reddit = pd.concat(chunks)\n",
    "print(reddit.shape)\n",
    "reddit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3de7f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit['corpus'] = 'reddit'\n",
    "reddit['language'] = 'english'\n",
    "reddit['speaker_type'] = 'L1'\n",
    "reddit.rename(columns={'content_len': 'num_words'}, inplace=True)\n",
    "reddit.drop(columns=['summary_len', 'subreddit', 'subreddit_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73594f7c",
   "metadata": {},
   "source": [
    "### Get length distribution & sample from datasets\n",
    "We round nuw_words up and treat each value as a separate class, then sample 20,000 texts from both Pikabu and Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e1f8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_classes(x):\n",
    "    if x < 500:\n",
    "        return 5 * round(x/5)\n",
    "    elif x < 1000:\n",
    "        return 10 * round(x/10)\n",
    "    else:\n",
    "        return 25 * round(x/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86a1374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_class'] = data['num_words'].map(round_to_classes)\n",
    "classes = data['num_class'].value_counts().reset_index().to_dict(orient='records')\n",
    "classes = {row['num_class']: row['count'] for row in classes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42c7de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pikabu['num_class'] = pikabu['num_words'].map(round_to_classes)\n",
    "reddit['num_class'] = reddit['num_words'].map(round_to_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e5a6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "psample = pikabu.sample(20000, random_state=RANDOM_STATE,\n",
    "                        weights=pikabu['num_class'].map(pd.Series(classes)/pikabu['num_class'].value_counts()))\n",
    "rsample = reddit.sample(20000, random_state=RANDOM_STATE,\n",
    "                        weights=reddit['num_class'].map(pd.Series(classes)/reddit['num_class'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "34464371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psample.to_json('_clean/pikabu.json', orient='records', force_ascii=False, indent=4)\n",
    "# rsample.to_json('_clean/reddit.json', orient='records', force_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98517c55",
   "metadata": {},
   "source": [
    "### Annotate Pikabu & Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a01130d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "psample = pd.read_json('_clean/pikabu.json', orient='records')\n",
    "rsample = pd.read_json('_clean/reddit.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "781abdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = create_raw_files(psample['text'].tolist(), corpus_name='pikabu', model=RUS_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a776310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK!\n"
     ]
    }
   ],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "conllus = check_annotated_files('pikabu', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce19b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psample = get_annotations(psample, conllus, '_annotated/pikabu.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f87ab2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>author_id</th>\n",
       "      <th>num_words</th>\n",
       "      <th>corpus</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>num_class</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2512869</td>\n",
       "      <td>Герман Обухов: Сбитая Россия!!!</td>\n",
       "      <td>В августе 1983 года во время правления\\nЮрия А...</td>\n",
       "      <td>2014-07-26 06:33:36</td>\n",
       "      <td>681246</td>\n",
       "      <td>926</td>\n",
       "      <td>pikabu</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>930</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3815335</td>\n",
       "      <td>Почему инопланетяне не выходят с нами на контакт</td>\n",
       "      <td>На дороге лежит червяк, и вы проходите мимо не...</td>\n",
       "      <td>2015-11-30 08:22:37</td>\n",
       "      <td>1128238</td>\n",
       "      <td>201</td>\n",
       "      <td>pikabu</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>200</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5530814</td>\n",
       "      <td>Ремарка про молодое поколение</td>\n",
       "      <td>Сделаю пост в качестве места тыка для тех кто ...</td>\n",
       "      <td>2017-12-03 17:03:41</td>\n",
       "      <td>704721</td>\n",
       "      <td>264</td>\n",
       "      <td>pikabu</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>265</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id                                             title  \\\n",
       "0  2512869                   Герман Обухов: Сбитая Россия!!!   \n",
       "1  3815335  Почему инопланетяне не выходят с нами на контакт   \n",
       "2  5530814                     Ремарка про молодое поколение   \n",
       "\n",
       "                                                text                date  \\\n",
       "0  В августе 1983 года во время правления\\nЮрия А... 2014-07-26 06:33:36   \n",
       "1  На дороге лежит червяк, и вы проходите мимо не... 2015-11-30 08:22:37   \n",
       "2  Сделаю пост в качестве места тыка для тех кто ... 2017-12-03 17:03:41   \n",
       "\n",
       "   author_id  num_words  corpus language speaker_type  num_class  \\\n",
       "0     681246        926  pikabu  russian           L1        930   \n",
       "1    1128238        201  pikabu  russian           L1        200   \n",
       "2     704721        264  pikabu  russian           L1        265   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psample = pd.read_json('_annotated/pikabu.json')\n",
    "psample.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1029e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = create_raw_files(rsample['text'].tolist(), corpus_name='reddit', model=ENG_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2948ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All OK!\n"
     ]
    }
   ],
   "source": [
    "# RUN .bat with Command Prompt, then the code below\n",
    "conllus = check_annotated_files('reddit', out_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda8317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsample = get_annotations(rsample, conllus, '_annotated/reddit.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c93da1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>author_id</th>\n",
       "      <th>corpus</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>num_class</th>\n",
       "      <th>annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>If you've visited the comments on any recent O...</td>\n",
       "      <td>The Ouya-hate phenomenon: one theory.</td>\n",
       "      <td>t3_1a8mz4</td>\n",
       "      <td>Ouya has received disproportionate criticism a...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>reddit</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>450</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256</td>\n",
       "      <td>Hi everyone, \\n I run a small web-site –  AGRO...</td>\n",
       "      <td>Request for volunteer writers for agricultural...</td>\n",
       "      <td>t3_1rint4</td>\n",
       "      <td>We run a website for farmers in developing cou...</td>\n",
       "      <td>AgroamTech</td>\n",
       "      <td>reddit</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>255</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>216</td>\n",
       "      <td>Firstly, I'd like to say how grateful I am to ...</td>\n",
       "      <td>[meta] Posts in this sub are starting to breac...</td>\n",
       "      <td>t3_220e9l</td>\n",
       "      <td>Plenty of videos do not seem to relate to Lit....</td>\n",
       "      <td>Wallstonecraft</td>\n",
       "      <td>reddit</td>\n",
       "      <td>english</td>\n",
       "      <td>L1</td>\n",
       "      <td>215</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_words                                               text  \\\n",
       "0        450  If you've visited the comments on any recent O...   \n",
       "1        256  Hi everyone, \\n I run a small web-site –  AGRO...   \n",
       "2        216  Firstly, I'd like to say how grateful I am to ...   \n",
       "\n",
       "                                               title     doc_id  \\\n",
       "0              The Ouya-hate phenomenon: one theory.  t3_1a8mz4   \n",
       "1  Request for volunteer writers for agricultural...  t3_1rint4   \n",
       "2  [meta] Posts in this sub are starting to breac...  t3_220e9l   \n",
       "\n",
       "                                              prompt       author_id  corpus  \\\n",
       "0  Ouya has received disproportionate criticism a...       [deleted]  reddit   \n",
       "1  We run a website for farmers in developing cou...      AgroamTech  reddit   \n",
       "2  Plenty of videos do not seem to relate to Lit....  Wallstonecraft  reddit   \n",
       "\n",
       "  language speaker_type  num_class  \\\n",
       "0  english           L1        450   \n",
       "1  english           L1        255   \n",
       "2  english           L1        215   \n",
       "\n",
       "                                           annotated  \n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...  \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsample = pd.read_json('_annotated/reddit.json')\n",
    "rsample.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5a685",
   "metadata": {},
   "source": [
    "## Combining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e711917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_annotated/actr.json',\n",
       " '_annotated/corst.json',\n",
       " '_annotated/locness.json',\n",
       " '_annotated/pikabu.json',\n",
       " '_annotated/realec.json',\n",
       " '_annotated/reddit.json',\n",
       " '_annotated/rlc.json',\n",
       " '_annotated/rulec.json']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = [os.path.join('_annotated/', file) for file in os.listdir('_annotated')]\n",
    "corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2f5605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac9b8f606a341bc80a9d5e70bf622d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corp_dfs = []\n",
    "for file in tqdm(corpora):\n",
    "    corpus = pd.read_json(file, orient='records')\n",
    "    corpus['corpus'], _ = os.path.splitext(os.path.basename(file))\n",
    "    corp_dfs.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0873a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49401, 66)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat(corp_dfs)\n",
    "data.dropna(how='all', axis=0, inplace=True)\n",
    "data.dropna(how='all', axis=1, inplace=True)\n",
    "data.fillna(np.nan, inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19f8b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'annotation': 'error_annotation',\n",
    "                     'corrected': 'corrected_text',\n",
    "                     'time': 'timed'}, inplace=True)\n",
    "data['study_year2'] = data['course']\n",
    "data.loc[~data['study_year2'].isin({'2 курс бак', '1 курс бак', '4 курс бак',\n",
    "                                    '2 курс спец', '3 курс бак'}), 'study_year2'] = np.nan\n",
    "data.loc[data['study_year2'].notnull(), 'course'] = np.nan\n",
    "data = data[~data['mode'].isin({'pairs', 'group'})]\n",
    "data.loc[data['subcorpus'] == 'RULEC', 'corpus'] = 'rulec'\n",
    "\n",
    "data['gender'] = data.apply(lambda x: combine_columns(x['gender'], x['sex']), axis=1)\n",
    "data['L1'] = data.apply(lambda x: combine_columns(x['native_lang'], x['native']), axis=1)\n",
    "data['author_name'] = data.apply(lambda x: combine_columns(x['name'], x['author'], x['student']), axis=1)\n",
    "data['institution'] = data.apply(lambda x: combine_columns(x['institution'], x['location'], x['university']), axis=1)\n",
    "data['doc_id'] = data.apply(lambda x: combine_columns(x['doc_id'], x['document_id']), axis=1)\n",
    "data['author_id'] = data.apply(lambda x: combine_columns(x['speaker_id'], x['code']), axis=1)\n",
    "data['prompt'] = data.apply(lambda x: combine_columns(x['title'], x['task'], x['topic'], x['prompt'], x['domain']), axis=1)\n",
    "data['text_type'] = data.apply(lambda x: combine_columns(x['text_type'], x['text type'], x['genre'], x['task_type']), axis=1)\n",
    "data['date'] = data.apply(lambda x: combine_columns(x['date'], x['date1'], x['date2'], x['year']), axis=1)\n",
    "data['study_year'] = data.apply(lambda x: combine_columns(x['study_year'], x['study_year2']), axis=1)\n",
    "data['programme'] = data.apply(lambda x: combine_columns(x['major'], x['course']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd200356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49397, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus</th>\n",
       "      <th>subcorpus</th>\n",
       "      <th>language</th>\n",
       "      <th>speaker_type</th>\n",
       "      <th>dialect</th>\n",
       "      <th>language_background</th>\n",
       "      <th>text</th>\n",
       "      <th>spell_checked</th>\n",
       "      <th>annotated</th>\n",
       "      <th>error_annotation</th>\n",
       "      <th>...</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>L1</th>\n",
       "      <th>level</th>\n",
       "      <th>institution</th>\n",
       "      <th>programme</th>\n",
       "      <th>study_year</th>\n",
       "      <th>term</th>\n",
       "      <th>module</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Прокатившись по многим городам и странам за по...</td>\n",
       "      <td>Прокатившись по многим городам и странам за по...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>В моей жизни я встречала много разных людей. У...</td>\n",
       "      <td>В моей жизни я встречала много разных людей. У...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>russian</td>\n",
       "      <td>L1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>В прошлем году, я провела четыре месяцев в Лон...</td>\n",
       "      <td>В прошлом году, я провела четыре месяцев в Лон...</td>\n",
       "      <td># generator = UDPipe 2, https://lindat.mff.cun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heritage 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus subcorpus language speaker_type dialect language_background  \\\n",
       "0   actr       NaN  russian           L1     NaN                 NaN   \n",
       "1   actr       NaN  russian           L1     NaN                 NaN   \n",
       "2   actr       NaN  russian           L1     NaN                 NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Прокатившись по многим городам и странам за по...   \n",
       "1  В моей жизни я встречала много разных людей. У...   \n",
       "2  В прошлем году, я провела четыре месяцев в Лон...   \n",
       "\n",
       "                                       spell_checked  \\\n",
       "0  Прокатившись по многим городам и странам за по...   \n",
       "1  В моей жизни я встречала много разных людей. У...   \n",
       "2  В прошлом году, я провела четыре месяцев в Лон...   \n",
       "\n",
       "                                           annotated error_annotation  ...  \\\n",
       "0  # generator = UDPipe 2, https://lindat.mff.cun...              NaN  ...   \n",
       "1  # generator = UDPipe 2, https://lindat.mff.cun...              NaN  ...   \n",
       "2  # generator = UDPipe 2, https://lindat.mff.cun...              NaN  ...   \n",
       "\n",
       "  gender  age   L1       level institution programme study_year term module  \\\n",
       "0    NaN  NaN  NaN  Heritage 1         NaN       NaN        NaN  NaN    NaN   \n",
       "1    NaN  NaN  NaN  Heritage 1         NaN       NaN        NaN  NaN    NaN   \n",
       "2    NaN  NaN  NaN  Heritage 1         NaN       NaN        NaN  NaN    NaN   \n",
       "\n",
       "  week  \n",
       "0  NaN  \n",
       "1  NaN  \n",
       "2  NaN  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['corpus', 'subcorpus', 'language', 'speaker_type', 'dialect',\n",
    "             'language_background', 'text', 'spell_checked', 'annotated',\n",
    "             'error_annotation', 'corrected_text', 'mark',\n",
    "             'text_type', 'prompt', 'function', 'date', 'timed',\n",
    "             'author_id', 'author_name', 'gender', 'age', 'L1', 'level',\n",
    "             'institution', 'programme', 'study_year', 'term', 'module', 'week']]\n",
    "print(data.shape)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "914e4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('SyntCompCorpus.tsv', sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
